{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import NeuralNet class\n",
    "from somlib import NeuralNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train neural network\n",
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "valid_data = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACC Macro</th>\n",
       "      <th>Bangdiwala B</th>\n",
       "      <th>Bennett S</th>\n",
       "      <th>Conditional Entropy</th>\n",
       "      <th>Cross Entropy</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>FNR Micro</th>\n",
       "      <th>FPR Micro</th>\n",
       "      <th>Gwet AC1</th>\n",
       "      <th>Hamming Loss</th>\n",
       "      <th>...</th>\n",
       "      <th>Reference Entropy</th>\n",
       "      <th>Response Entropy</th>\n",
       "      <th>Standard Error</th>\n",
       "      <th>TNR Micro</th>\n",
       "      <th>TPR Micro</th>\n",
       "      <th>mark_1</th>\n",
       "      <th>mark_2</th>\n",
       "      <th>mark_3</th>\n",
       "      <th>mark_4</th>\n",
       "      <th>mark_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.997977</td>\n",
       "      <td>0.997977</td>\n",
       "      <td>0.995955</td>\n",
       "      <td>0.021018</td>\n",
       "      <td>0.002921</td>\n",
       "      <td>0.997977</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.997973</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.021018</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.997977</td>\n",
       "      <td>0.997977</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998312</td>\n",
       "      <td>0.998309</td>\n",
       "      <td>0.996624</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.027589</td>\n",
       "      <td>0.998312</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.998306</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026095</td>\n",
       "      <td>0.010551</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.998312</td>\n",
       "      <td>0.998312</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.892172</td>\n",
       "      <td>0.878774</td>\n",
       "      <td>0.784344</td>\n",
       "      <td>0.487271</td>\n",
       "      <td>0.398881</td>\n",
       "      <td>0.892172</td>\n",
       "      <td>0.107828</td>\n",
       "      <td>0.107828</td>\n",
       "      <td>0.865341</td>\n",
       "      <td>0.107828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324197</td>\n",
       "      <td>0.646727</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.892172</td>\n",
       "      <td>0.892172</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ACC Macro  Bangdiwala B  Bennett S  Conditional Entropy  Cross Entropy  \\\n",
       "0   0.997977      0.997977   0.995955             0.021018       0.002921   \n",
       "1   0.998312      0.998309   0.996624             0.002434       0.027589   \n",
       "2   0.892172      0.878774   0.784344             0.487271       0.398881   \n",
       "\n",
       "   F1 Micro  FNR Micro  FPR Micro  Gwet AC1  Hamming Loss  ...  \\\n",
       "0  0.997977   0.002023   0.002023  0.997973      0.002023  ...   \n",
       "1  0.998312   0.001688   0.001688  0.998306      0.001688  ...   \n",
       "2  0.892172   0.107828   0.107828  0.865341      0.107828  ...   \n",
       "\n",
       "   Reference Entropy  Response Entropy  Standard Error  TNR Micro  TPR Micro  \\\n",
       "0          -0.000000          0.021018        0.000025   0.997977   0.997977   \n",
       "1           0.026095          0.010551        0.000023   0.998312   0.998312   \n",
       "2           0.324197          0.646727        0.000175   0.892172   0.892172   \n",
       "\n",
       "   mark_1  mark_2  mark_3  mark_4  mark_5  \n",
       "0       1       0       0       0       0  \n",
       "1       0       0       0       0       1  \n",
       "2       0       0       0       1       0  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define architecture. Architecture of neural network defining by `dict` object. \n",
    "\n",
    "Every `key` of this `dict` is name of layer, e.g. \"input\", \"layer_1\", \"first layer\", ect. This names choose by user. For each layer (`key`) `value` is another `dict`, vith keys:\n",
    "* `type`: layer type, required key; now available: \n",
    " * `\"fully_connected\"` - for fully-connected layer;\n",
    " * `\"convolution\"` - for convolution layer;\n",
    " * `\"max_pool\"` - for max pooling layer;\n",
    " * `\"flatten\"` - for reshape n-demetion tensors (for example: output of convolution layer) into vector (one-dimention object). For batch of objects reshape all objects in batch, but save first dimention $(10 \\times 5 \\times 3 \\times 2) \\rightarrow (10 \\times 30)$\n",
    " * `\"out\"` - for last fully-connected layer (e.g. output of model)\n",
    "* \"`activation\"` - non-lineary function for all layers, <u>except</u> **max pooling** and **flatten**, required key; now available: \n",
    " * `\"sigmoid\"`:\n",
    " $$ h_ \\theta (x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^- \\theta^Tx }  $$\n",
    " * `\"tanh\"`:\n",
    " $$ tanh(x) = \\frac{e^{2x} - 1}{e^{2x} + 1}$$\n",
    " * `\"relu\"`:\n",
    " $$ReLU(x) = max(x, 0)$$\n",
    " * `\"softmax\"`:\n",
    " $$\\sigma (x)_{i} = \\frac{e^{x_{i}}}{\\sum^{K}_{k=1}{e^{x_{k}}}}$$\n",
    "* specific keys for **convolution layer**:\n",
    " * `\"filtres\"`: number of filters in convolution layer, dtype: `int`;\n",
    " * `\"kernel\"` : shape of filter (convolution kernel), this key recive list of 2 int for \"width\" and \"heigh\" of convolution kernel, dtype: `list`;\n",
    " * `\"stride\"` : stride along \"width\" and \"heigh\" for convolution operation, this key recive list of 2 int for \"width\" and \"heigh\" stride, dtype: `list`;\n",
    " * `\"pad\"` : padding input tensor with zeros along \"width\" and \"heigh\", this key recive list of 2 int for \"width\" and \"heigh\" stride, dtype: `list`;\n",
    "* specific keys for **max pooling layer**:\n",
    " * `\"kernel\"` : shape of max pooling mask, this key recive list of 2 int for \"width\" and \"heigh\" of max pooling mask, dtype: `list`;\n",
    " * `\"stride\"` : stride along \"width\" and \"heigh\" for max pooling, this key recive list of 2 int for \"width\" and \"heigh\" stride, dtype: `list`;\n",
    "* specific keys for **fully-connected layer**:\n",
    " * `\"neurons\"`: number of hidden units (neurons) into layer, dtype: `int`\n",
    "* **flatten layer** has no parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = {\n",
    "    \"l1\": {\"type\": \"fully_conneted\", \"neurons\": 31, \"activation\": \"sigmoid\"},\n",
    "    \"l2\": {\"type\": \"fully_conneted\", \"neurons\": 18, \"activation\": \"sigmoid\"},\n",
    "    \"out\": {\"type\": \"out\", \"neurons\": 5, \"activation\": \"sigmoid\"},\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
